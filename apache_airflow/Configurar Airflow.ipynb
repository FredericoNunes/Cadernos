{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Airflow\n",
    "\n",
    "Primeiros passos recomendo esse tutorial\n",
    "\n",
    "https://medium.com/data-hackers/primeiros-passos-com-o-apache-airflow-etl-f%C3%A1cil-robusto-e-de-baixo-custo-f80db989edae\n",
    " \n",
    "Esse tutorial abaixo irá falar sobre o Air flow no Google Componser\n",
    "\n",
    "https://www.youtube.com/watch?v=ld6JO3MiuPQ\n",
    "\n",
    "O kubernets é configurado sem muita dor de cabeça!!! Eu realmente consegui em um dia.\n",
    "\n",
    "O Conceito central está na DAG = Directed Acyclic Graph (DAG)\n",
    "O segredo pra montar uma DAG é seguir 5 passos:\n",
    "    1. Importar Modulos\n",
    "    2. Setar os argumentos Default\n",
    "    3. Instanciar a DAG\n",
    "    4. Apontar as Tasks\n",
    "    5. Montar Dependencias\n",
    "    \n",
    "Tutorial:\n",
    "https://airflow.apache.org/docs/stable/tutorial.html\n",
    "\n",
    "Quase tudo nesse tutorial é diregivel de forma rápida, menos o Jinja Template que no final server para melhor apresentar o print dos testes na tela do Log\n",
    "https://jinja.palletsprojects.com/en/2.11.x/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O Site oficial passa o guia sobre ambiente em produção\n",
    "https://airflow.apache.org/docs/stable/howto/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://airflow.apache.org/docs/stable/howto/set-config.html\n",
    "1. Configuração inicial do arquivo airflow.cfg => aqui configura o banco de dados\n",
    "2. Ou Configurar via variaveis de ambiente (melhor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trocar Banco de Dados\n",
    "\n",
    "https://airflow.apache.org/docs/stable/howto/initialize-database.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O Conceito de Operators\n",
    "\n",
    "https://airflow.apache.org/docs/stable/howto/operator/index.html\n",
    "\n",
    "\n",
    "https://airflow.apache.org/docs/stable/_api/index.html\n",
    "\n",
    "Um operador representa uma única tarefa, idealmente idempotente. Os operadores determinam o que realmente é executado quando o DAG é executado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curso\n",
    "\n",
    "https://www.udemy.com/course/the-ultimate-hands-on-course-to-master-apache-airflow/learn/lecture/15819368#overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ate o sexto video nada de novo. A Apartir do sétimo:\n",
    "\n",
    "    Componentes principais do framework Airflow:\n",
    "        1. Web server = flask server with guincorn\n",
    "        2. Scheduler = daemon \n",
    "        3. Metadata database = o db tem que ser compativel com SQLAlchemy\n",
    "        4. Executor = define como uma task deve ser executada\n",
    "        5. worker = processo que executa as tasks\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceitos \n",
    "    DAG\n",
    "    Operator\n",
    "    Task\n",
    "    TaskInstance\n",
    "    workflow\n",
    "    \n",
    "## Como Funciona\n",
    "    1. O Scheduler lê a pasta DAG\n",
    "    2. O DAG é parsiado por um processo para criar um DagRun baseado nos parametros de agenda dentro do DAG\n",
    "    3. Uma TaskInstance é instanciada por cada task que precisa ser executada a marcada \"Scheduled\" no bando de      dados do metadata.\n",
    "    4. O Scheduler pega todas as TaskInstances marcadas com \"Scheduled\" do banco de dados do metadata, muda o status para \"Queued\" e manda as TaskInstances para os Executors\n",
    "    5. Executors retiram (pull out) Tasks da fila (dependendo do setup de execução), muda o status de \"Queued\" para \"Running\" e Workers começam a executar as TaskInstances. \n",
    "    6. Quando a Task é terminada o Executor muda o status daquela task para o status final (success, failed,etc) no banco de dados e o DAGRun é atualizado pelo Scheduler com o status \"Success\" ou \"Falho\". o webserver periodicamente fetch dados do bando de dado do metadata e atualiza o UI\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O video 10 detalhas o UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video 11 maioria dos comandos usados no CLI do Airflow\n",
    "    1. airflow initdb\n",
    "    2. airflow resetdb #deleta td\n",
    "    3. airflow upgradedb #atualizar tabelas quando sair um novo airflow\n",
    "    4. airflow webserver -p -w \n",
    "    5. airflow scheduler #voce poder mudar a pasta dos dags\n",
    "    6. airflow worker # Start o celery executor (celery é um distribuidor de task) ajudda a scalr o airflow\n",
    "    7. airflow trigger_dag <nome_dag> # se usar -e 2020-01-01 irá retroagir o dag ate essa data e executará ate o dia atual\n",
    "    8. airflow list_dags_runs\n",
    "    9. airflow test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video 14 Docker file e file Composer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 5\n",
    "\n",
    "Video 47 Após Mostrar como manipular as DAG's apartir daqui o curso mostra \n",
    "a escalabilidade do Airflow e começa a diferenciação entre:\n",
    "\n",
    "    1. Sequencial Executor, (Padrão)\n",
    "    2. Local Executor, (Abre para  paralelismo)\n",
    "    3. Celery Executor, (Resolve a execução distribuida horizontal scaling)\n",
    "    4. Kubernets Executor, (Resolve a a orquestração em cluster, resolve ponto unico de falha)\n",
    " \n",
    "O sequencial usa o bando de dados sqlite\n",
    "Para fazer o local executor é melhor usar um RDBMS, Postgres\n",
    "Os outros o nome ja inidca a ideia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Videos 49 e 50 Local Executor com Postgres \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video 50 Adhoc query de dados. Remover essa opção com o secure_mode = True no airflow.cfg\n",
    "\n",
    "Airflow não é um explorador de banco de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video 50 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diario de Bordo da Configuração com Dockcer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Criei Intancia AWS\n",
    "2) Criei Banco de Dados Postgres\n",
    "        (Preciso pensar se o bando de dados será      proprio ou compartilhado)\n",
    "3) Entrei na instancia AWS pra installar o docker\n",
    "    Tutorial nesse link => https://docs.docker.com/engine/install/ubuntu/#install-from-a-package (Instalei a Versão com repositorio)\n",
    "    \n",
    "4) Também instalei localmente.\n",
    "    Docker Manual Link = > https://docs.docker.com/get-started/part2/\n",
    "        a) Criar Docker file\n",
    "        b) docker build --tag nome:versao (i.e airflowserver:1.0) . (lembra do ponto como endereço onde está o docker file\n",
    "        c) docker run --publish 8000:8080 --detach --name arflw --env-file .env --mount type=bind,source=/home/frederico/a552020/Dags,target=/usr/local/airflow/dags airflow:1.0\n",
    "            comando pra exercutar o servidor dentro do docker\n",
    "            >> publish altera as portas (8000 é a q que vem do host e 8080 é a port do serviço detro do container)\n",
    "            >> detach é rodar em backgroud\n",
    "            >> name é colocar uma tag no run\n",
    "        d) docker ps \n",
    "        e) docker system prune (Use esse comando somente na sua maquina local e olhe lá.\n",
    "            Isso apagará tudo.\n",
    "        f) docker stop $(docker ps -a -q)\n",
    "        g) docker container ls -a\n",
    "        h) docker exec -it $(docker container ls -q) /bin/bash\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIT Nova key\n",
    "ssh-add -l\n",
    "ssh-keygen -t rsa -b 4096 -C \"joe@example.com\"\n",
    "cat .ssh/id_rsa.pub\n",
    "https://www.inmotionhosting.com/support/website/ssh/how-to-add-ssh-keys-to-your-github-account/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
